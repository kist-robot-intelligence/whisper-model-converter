{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파인튜닝된 huggingface 기반 모델을 faster whisper로 바꾸는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★★★★★★★★★★ 1. 합칠 어댑터 경로 입력(이건 반드시 수정) ★★★★★★★★★★\n",
    "# 보통 /home/kist/fast_whisper/가_ipynb_collection에 있는 수정본.ipynb 파일로 파인튜닝 시키고 그 결과로 adapter가 생성되기 때문에 대부분은 이 경로 안에 있을 것임.\n",
    "# peft_model_id_offline = '[adapter_config.json, adapter_model.safetensors, README.md 가 있는 디렉토리]'\n",
    "peft_model_id_offline = '/home/kist/fast_whisper/가_ipynb_collection/03261858'\n",
    "\n",
    "# base model 경로(이거는 이 컴퓨터에서 위치 바꾸지 않는 이상 바꿀 필요 없음)\n",
    "# base_model_name_or_path = '[config.json, tokenizer.json, 기타 모델 파일 등이 들어 있는 디렉토리]'\n",
    "base_model_name_or_path = '/home/kist/fast_whisper/openai_whisperLargeV3/models--openai--whisper-large-v3/snapshots/1ecca609f9a5ae2cd97a576a9725bc714c022a93'\n",
    "\n",
    "from transformers import (\n",
    "    AutomaticSpeechRecognitionPipeline,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "#device = 'cpu'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "language = \"Korean\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id_offline)\n",
    "\n",
    "\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    base_model_name_or_path, load_in_8bit=False, device_map = 0\n",
    "    ).to(device)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id_offline).to(device) #using adapter\n",
    "\n",
    "# ---merging model and peft adapter---\n",
    "\n",
    "now = datetime.now()\n",
    "plus = now.strftime(\"%m%d%H%M\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained('merged_model_' + plus) # 이 경로에 adapter와 base model이 합쳐진 모델이 저장됨.\n",
    "\n",
    "print('adapter 병합 완료!!!')\n",
    "\n",
    "folder_name = './files_for_converting_to_faster_whisper'\n",
    "filelist = os.listdir(folder_name)\n",
    "dest_folder = '.'\n",
    "\n",
    "for fl in filelist:\n",
    "    shutil.copy(folder_name+'/'+fl, dest_folder+'/merged_model_'+plus+'/'+fl)\n",
    "\n",
    "inst1 = 'ct2-transformers-converter --model merged_model_' + plus + ' --output_dir faster_merged_model_' + plus + ' --copy_files tokenizer.json preprocessor_config.json --quantization float16'\n",
    "inst2 = 'rm -rf merged_model_' + plus\n",
    "os.system(inst1)\n",
    "os.system(inst2)\n",
    "\n",
    "print('faster whisper 변환 완료!!!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 faster_merged_model_03261640과 같은 faster_whisper 기반의 모델 파일들을 담고 있는 디렉토리가 생성될 것임."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jh_temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
